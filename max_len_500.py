# -*- coding: utf-8 -*-
"""max_len_500.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Fp-uTlsHL-2Nq9e4PFplnTPv8atJ_op
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import glob

import pandas as pd
import os, sys, email
import numpy as np 
import pandas as pd

# Plotting
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns; sns.set_style('whitegrid')
import wordcloud

# Network analysis
import networkx as nx

# NLP
from nltk.tokenize.regexp import RegexpTokenizer

# # extracting the downloaded data "A subset of about 1700 labelled email messages" from https://bailando.berkeley.edu/enron_email.html
# # importing the "tarfile" module
# import tarfile
  
# # open file
# file = tarfile.open('/content/drive/MyDrive/vlabs/enron_with_categories.tar.gz')
  
# # extracting file
# file.extractall('/content/drive/MyDrive/vlabs')
  
# file.close()


import numpy as np
import pandas as pd
import string
import itertools
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook as tqdm

from gensim.models import *

import tensorflow as tf
from tensorflow.keras.utils import *
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
import tensorflow.keras.backend as K
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder

import string, re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer

#Reading the paths of the data of all 6 categories
email_1 = glob.glob('/content/drive/MyDrive/vlabs/enron_with_categories/1/*.txt')
email_2 = glob.glob('/content/drive/MyDrive/vlabs/enron_with_categories/2/*.txt')
email_3 = glob.glob('/content/drive/MyDrive/vlabs/enron_with_categories/3/*.txt')
email_4 = glob.glob('/content/drive/MyDrive/vlabs/enron_with_categories/4/*.txt')
email_5 = glob.glob('/content/drive/MyDrive/vlabs/enron_with_categories/5/*.txt')
email_6 = glob.glob('/content/drive/MyDrive/vlabs/enron_with_categories/6/*.txt')

category_len = [] #this list will store number of emails belonging to each category
category_len.append(len(email_1))
category_len.append(len(email_2))
category_len.append(len(email_3))
category_len.append(len(email_4))
category_len.append(len(email_5))
category_len.append(len(email_6))

category_len

#function to read emails when the path is given
def read_email(pathList):
  temp = []
  for path in pathList:
    with open(path) as f:
      lines = f.read()
      temp.append(lines)
  return temp

emailData = [] #list to store all emails

emailData = read_email(email_1) + read_email(email_2) + read_email(email_3) + read_email(email_4) + read_email(email_5) + read_email(email_6)

emailData[0]

# # Code To check if all the email files are read into the emailData
# res = 0
# for i in range(len(category_len)):
#   res += category_len[i]

# print(len(emailData))
# print(res)

# We can read more about email parser from https://docs.python.org/3/library/email.parser.html


def get_text_from_email(message):
    '''To get the content from email objects'''
    parts = []
    for part in message.walk():
        if part.get_content_type() == 'text/plain':
            parts.append( part.get_payload() )
    return ''.join(parts)

def split_email_addresses(line):
    '''To separate multiple email addresses'''
    if line:
        address = line.split(',')
        address = frozenset(map(lambda x: x.strip(), address))
    else:
        address = None
    return address

# Parse the emails into a list email objects
#remember that emailData is a list comtaining all emails
# email.message_from_string returns a message object structure from a string

messages = list(map(email.message_from_string, emailData))

keys = messages[0].keys()
print(keys)

#creating a pandas dataframe to store all the data of emails

emails_df = pd.DataFrame()

for key in keys:
    emails_df[key] = [doc[key] for doc in messages]

#storing the content of the emails in a separate column

emails_df['Content'] = list(map(get_text_from_email, messages))

# Split multiple email addresses
emails_df['From'] = emails_df['From'].map(split_email_addresses)
emails_df['To'] = emails_df['To'].map(split_email_addresses)

del messages

emails_df.head()

emails_sample_df = emails_df[['Subject','Content']]

emails_sample_df.head()

#creating labels column

label = []
for i in range(len(category_len)):
  label = label + [i+1] * category_len[i] #since the emails are stored in an arrangement (category1,category2,...,category6),therefore storing the labels accordingly

print(len(label))

#adding the labels to dataframe
emails_sample_df['label'] = label

emails_sample_df

"""# Preprocessing of the subject and the content of emails"""

#concatenate the subject and content of emails into a single data element
emails_sample_df['data'] = emails_sample_df['Subject'] + emails_sample_df['Content']
emails_sample_df.head()

#changing the ordering of columns
emails_sample_df = emails_sample_df[['Subject','Content','data','label']]
emails_sample_df.head()

#convert all words to lowercase
emails_sample_df["data"] = emails_sample_df["data"].str.lower()

#defining the function to remove punctuation
def remove_punctuation(text):
    temp ="".join([z for z in text if z not in string.punctuation])
    return temp

emails_sample_df['data'] = emails_sample_df['data'].apply(lambda text: remove_punctuation(text))


STOPWORDS = set(stopwords.words('english'))
def remove_stopwords(text):
    """custom function to remove the stopwords"""
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

emails_sample_df['data'] = emails_sample_df['data'].apply(lambda text: remove_stopwords(text))

lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

emails_sample_df['data'] = emails_sample_df['data'].apply(lambda text: lemmatize_words(text))

"""# Splitting the dataset into train and test data"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(emails_sample_df['data'], emails_sample_df['label'], test_size=0.20, random_state=42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""# DATA VISUALIZATION"""

#TO STORE LENGTH OF ALL THE EMAILS FOR VISUALIZATION PURPOSE
text_len = list()

# text_len = emails_sample_df['data'].str.split().str.len()
text_len = X_train.str.split().str.len()

text_len

#TO SEE THE MAXIMUM LENGTH OF AN EMAIL IN THE training DATA
max(text_len)

import matplotlib.pyplot as plt
x =  list(range(0, len(text_len)))

plt.plot(x, text_len)
 
# naming the x axis
plt.xlabel('EMAIL ID')
# naming the y axis
plt.ylabel('LENGTH OF EMAIL')
 
# giving a title to my graph
plt.title('')
 
# function to show the plot
plt.show()

#THE ABOVE PLOT DOESN'T GIVE US A CLEAR IDEA OF THE LENGTH OF THE EMAILS, SO NOW WE WILL MOVE TO HISTOGRAM FOR BETTER VISUALIZATION

bins_1 = np.arange(0,max(text_len),100) #starting point is zero, ending point is maximum length pf email, and step size is 250
#print(bins_1)

# Creating histogram
fig, ax = plt.subplots(figsize =(10, 7))
ax.hist(text_len, bins_1)
 
# Show plot
plt.show()

#for a better visualization, we change the end point to 1001, and also increase the step size
bins = np.arange(0,1001,250)
#print(bins)

# Creating histogram
fig, ax = plt.subplots(figsize =(10, 7))
ax.hist(text_len, bins)
 
# Show plot
plt.show()

"""*We set up the maximum length of an email data to be equal to 750, since approximately 90% of the emails have maximum length equal to 750*"""

max_len_data = 500

"""# GloVe"""

import gensim
import gensim.downloader

glove = gensim.downloader.load('glove-wiki-gigaword-300')

max_len = max_len_data
tokenizer = Tokenizer(lower=True)
tokenizer.fit_on_texts(X_train) #Updates internal vocabulary based on a list of texts

sequence_train = tokenizer.texts_to_sequences(X_train) #Transforms each text in texts to a sequence of integers.
sequence_train = pad_sequences(sequence_train, maxlen=max_len)

# Read more about tokenization from https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer

sequence_test = tokenizer.texts_to_sequences(X_test)
sequence_test = pad_sequences(sequence_test, maxlen=max_len)


sequence_train = np.asarray(sequence_train)

sequence_test = np.asarray(sequence_test)

word_index = tokenizer.word_index #word_index is dictionary of words
vocab_size = len(word_index)+1

print(vocab_size)
print(sequence_train.shape, sequence_test.shape)

embedding_matrix_glove = np.random.random((len(tokenizer.word_index)+1, 300))

pas = 0
for word,i in tokenizer.word_index.items():
    
    try:
        embedding_matrix_glove[i] = glove.wv[word]
    except:
        pas+=1
        
print('not in vocabulary', pas)

embedding_matrix_glove.shape

y_train_df = pd.DataFrame()
y_train_df['label'] = y_train
y_train_df.head()

enc = OneHotEncoder(handle_unknown='ignore')
#performing one hot encoding

enc_df = pd.DataFrame(enc.fit_transform(y_train_df).toarray())

enc_df

# x_train = emails_sample_df['data']
x_train_df = pd.DataFrame(X_train)
y_train_label = enc_df
y_train_label = y_train_label.to_numpy()
print(x_train_df)
print(y_train_label)

embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_glove], input_length=max_len_data, trainable=False)

glove_model = Sequential()
glove_model.add(embedding_layer)
glove_model.add(Conv1D(filters=128, kernel_size=6, activation='relu'))
glove_model.add(MaxPooling1D(pool_size=2))
glove_model.add(LSTM(64))
glove_model.add(Flatten())
#glove_model.add(Dense(20, activation='relu'))
glove_model.add(Dense(6, activation='softmax'))
print(glove_model.summary())

from tensorflow import keras
from tensorflow.keras import layers

opt = keras.optimizers.Adam(learning_rate=0.0005)
glove_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

checkpoint_filepath = '/content/sample_data/Glove_Model'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

history = glove_model.fit(sequence_train, y_train_label, epochs=10, verbose=1, validation_split =0.10,  callbacks=[model_checkpoint_callback])

train_loss = history.history['loss']
train_accuracy = history.history['accuracy']

val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

epochs = np.arange(len(train_loss))

plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Losses')

plt.xlabel('epochs')
plt.ylabel('Loss')

plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')

plt.show()

glove_model = tf.keras.models.load_model(checkpoint_filepath)

predictions_glove = glove_model.predict(sequence_test)
predictions_glove

y_test_df = pd.DataFrame()
y_test_df['label'] = y_test
y_test_df.head()
test_label = pd.DataFrame(enc.fit_transform(y_test_df).toarray())

predicted_glove = []
for i in predictions_glove:
  predicted_glove.append(np.argmax(i))

predicted_glove = np.array(predicted_glove)
predicted_glove

import sklearn.preprocessing
label_binarizer = sklearn.preprocessing.LabelBinarizer()
label_binarizer.fit(range(max(predicted_glove)+1))
predicted_glove = label_binarizer.transform(predicted_glove)
print(predicted_glove)

predicted_glove.shape

from sklearn import metrics

test_loss_glove, test_acc_glove = glove_model.evaluate(sequence_test, test_label)
print('Test Loss:', test_loss_glove)
print('Test Accuracy:', test_acc_glove)
print(metrics.f1_score(test_label, predicted_glove, average='macro'))



"""# Fasttext"""

ftext = gensim.downloader.load('fasttext-wiki-news-subwords-300')

embedding_matrix_ft = np.random.random((len(tokenizer.word_index)+1 , 300))

pas = 0
for word,i in tokenizer.word_index.items():
    
    try:
        embedding_matrix_ft[i] = ftext.wv[word]
    except:
        pas+=1
        
print('not in vocabulary', pas)

embedding_layer_ftext = Embedding(vocab_size, 300, weights=[embedding_matrix_ft], input_length=max_len_data, trainable=False)

ftext_model = Sequential()
ftext_model.add(embedding_layer_ftext)
ftext_model.add(Conv1D(filters=128, kernel_size=6, activation='relu'))
ftext_model.add(MaxPooling1D(pool_size=2))
ftext_model.add(LSTM(64))
ftext_model.add(Flatten())
#glove_model.add(Dense(20, activation='relu'))
ftext_model.add(Dense(6, activation='softmax'))
print(ftext_model.summary())

from tensorflow import keras
from tensorflow.keras import layers

opt = keras.optimizers.Adam(learning_rate=0.0005)
ftext_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

checkpoint_filepath_ftext = '/content/sample_data/Ftext_Model'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath_ftext,
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

history = ftext_model.fit(sequence_train, y_train_label, epochs=10, verbose=1, validation_split =0.10,  callbacks=[model_checkpoint_callback])

train_loss = history.history['loss']
train_accuracy = history.history['accuracy']

val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

epochs = np.arange(len(train_loss))

plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Losses')

plt.xlabel('epochs')
plt.ylabel('Loss')

plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')

plt.show()

ftext_model = tf.keras.models.load_model(checkpoint_filepath_ftext)

predictions_ftext = ftext_model.predict(sequence_test)
predictions_ftext



predicted_ftext = []
for i in predictions_ftext:
  predicted_ftext.append(np.argmax(i))

predicted_ftext = np.array(predicted_ftext)
predicted_ftext



label_binarizer_ftext = sklearn.preprocessing.LabelBinarizer()
label_binarizer_ftext.fit(range(max(predicted_ftext)+1))
predicted_ftext = label_binarizer_ftext.transform(predicted_ftext)
print(predicted_ftext)


test_loss_ftext, test_acc_ftext = ftext_model.evaluate(sequence_test, test_label)
print('Test Loss:', test_loss_ftext)
print('Test Accuracy:', test_acc_ftext)
print(metrics.f1_score(test_label, predicted_ftext, average='macro'))

pip install tensorflow

import tensorflow
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense

"""# CNN ONLY GloVe"""

embed_len = 300

inputs = Input(shape=(max_len_data, ))
embedding_layer_glove = Embedding(vocab_size, 300, weights=[embedding_matrix_glove], input_length=max_len_data, trainable=False)
conv = Conv1D(32, 7, padding="same") ## Channels last
dense = Dense(6, activation="softmax")

x = embedding_layer_glove(inputs)
x = conv(x)
x = tensorflow.reduce_max(x, axis=1)
output = dense(x)

glove_model_cnn = Model(inputs=inputs, outputs=output)

glove_model_cnn.summary()

opt = keras.optimizers.Adam(learning_rate=0.0005)
glove_model_cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

history = glove_model_cnn.fit(sequence_train, y_train_label, epochs=20, verbose=1, validation_split =0.10)

train_loss = history.history['loss']
train_accuracy = history.history['accuracy']

val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

epochs = np.arange(len(train_loss))

plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Losses')

plt.xlabel('epochs')
plt.ylabel('Loss')

plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')

plt.show()


predictions_glove_cnn = glove_model_cnn.predict(sequence_test)
predictions_glove_cnn



predicted_glove_cnn = []
for i in predictions_glove_cnn:
  predicted_glove_cnn.append(np.argmax(i))

predicted_glove_cnn = np.array(predicted_glove_cnn)
print(predicted_glove_cnn)



label_binarizer_glove_cnn = sklearn.preprocessing.LabelBinarizer()
label_binarizer_glove_cnn.fit(range(max(predicted_glove_cnn)+1))
predicted_glove_cnn = label_binarizer_glove_cnn.transform(predicted_glove_cnn)
print(predicted_glove_cnn)


test_loss_glove_cnn, test_acc_glove_cnn = glove_model_cnn.evaluate(sequence_test, test_label)
print('Test Loss:', test_loss_glove_cnn)
print('Test Accuracy:', test_acc_glove_cnn)
print(metrics.f1_score(test_label, predicted_glove_cnn, average='macro'))



"""# CNN Ftext only"""

embed_len = 300

inputs = Input(shape=(max_len_data, ))
embedding_layer_ft = Embedding(vocab_size, 300, weights=[embedding_matrix_ft], input_length=max_len_data, trainable=False)
conv = Conv1D(32, 7, padding="same") ## Channels last
dense = Dense(6, activation="softmax")

x = embedding_layer_ft(inputs)
x = conv(x)
x = tensorflow.reduce_max(x, axis=1)
output = dense(x)

ftext_model_cnn = Model(inputs=inputs, outputs=output)

ftext_model_cnn.summary()

opt = keras.optimizers.Adam(learning_rate=0.0005)
ftext_model_cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

history = ftext_model_cnn.fit(sequence_train, y_train_label, epochs=20, verbose=1, validation_split =0.10)

train_loss = history.history['loss']
train_accuracy = history.history['accuracy']

val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

epochs = np.arange(len(train_loss))

plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Losses')

plt.xlabel('epochs')
plt.ylabel('Loss')

plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')

plt.show()


predictions_ftext_cnn = ftext_model_cnn.predict(sequence_test)
predictions_ftext_cnn



predicted_ftext_cnn = []
for i in predictions_ftext_cnn:
  predicted_ftext_cnn.append(np.argmax(i))

predicted_ftext_cnn = np.array(predicted_ftext_cnn)
print(predicted_ftext_cnn)



label_binarizer_ftext_cnn = sklearn.preprocessing.LabelBinarizer()
label_binarizer_ftext_cnn.fit(range(max(predicted_ftext_cnn)+1))
predicted_ftext_cnn = label_binarizer_ftext_cnn.transform(predicted_ftext_cnn)
print(predicted_ftext_cnn)


test_loss_ftext_cnn, test_acc_ftext_cnn = ftext_model_cnn.evaluate(sequence_test, test_label)
print('Test Loss:', test_loss_ftext_cnn)
print('Test Accuracy:', test_acc_ftext_cnn)
print(metrics.f1_score(test_label, predicted_ftext_cnn, average='macro'))